{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8a8c6c",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b18df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff01fae",
   "metadata": {},
   "source": [
    "### Scraping Following List using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfc059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter usernames to scrape\n",
    "twitter_usernames = ['CreativeScots', 'BordersBookFest', 'TradDanceScot', 'scottishmusic', 'GlasgowTramway', 'makemanifesto', 'summerhallery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape followed accounts\n",
    "def scrape_following(usernames):\n",
    "    # Launch Chrome browser\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open Twitter login page\n",
    "    driver.get(\"https://twitter.com/login\")\n",
    "    # Wait for some time to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Locate and fill in the username field\n",
    "    username_field = driver.find_element(By.NAME, \"text\")\n",
    "    username_field.send_keys(\"USERNAME\")                       ######## Replace with your Twitter username\n",
    "    # Submit the login form\n",
    "    username_field.send_keys(Keys.RETURN)\n",
    "    # Wait for login to complete\n",
    "    time.sleep(2)\n",
    "    \n",
    "    ### Use this commented part of code if twitter asks additionally for email/phone number\n",
    "    \n",
    "#     # Locate and fill in the username field\n",
    "#     username_field = driver.find_element(By.NAME, \"text\")\n",
    "#     username_field.send_keys(\"EMAIL/PHONE NUMBER\")           ######## Replace with your Twitter email/phone number\n",
    "#     # Submit the login form\n",
    "#     username_field.send_keys(Keys.RETURN)\n",
    "#     # Wait for login to complete\n",
    "#     time.sleep(2)\n",
    "\n",
    "    # Locate and fill in the password field\n",
    "    password_field = driver.find_element(By.NAME, \"password\")\n",
    "    password_field.send_keys(\"PASSWORD\")                         ######## Replace with your Twitter password\n",
    "    # Submit the login form\n",
    "    password_field.send_keys(Keys.RETURN)\n",
    "    # Wait for login to complete\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Extracting the following accounts by navigating to users following page\n",
    "    following_accounts_dict = {}\n",
    "    for user in usernames:\n",
    "        # Navigate to the target user's following page\n",
    "        driver.get(f\"https://twitter.com/{user}/following\")\n",
    "        # Wait for the following page to load\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Height of the viewport\n",
    "        height_viewport = driver.execute_script(\"return window.innerHeight;\")\n",
    "\n",
    "        # Assign the length of the page to a variable before scrolling down\n",
    "        last_height = len(WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.XPATH, '//div[@data-testid=\"cellInnerDiv\"]//div[@class=\"css-146c3p1 r-dnmrzs r-1udh08x r-3s2u2q r-bcqeeo r-1ttztb7 r-qvutc0 r-37j5jr r-a023e6 r-rjixqe r-16dba41 r-18u37iz r-1wvb978\"]'))))\n",
    "        following_accounts =[]\n",
    "        while True:\n",
    "            try:\n",
    "                # Scrolling down the following accounts page\n",
    "                driver.execute_script(f\"window.scrollBy(0, {height_viewport});\")\n",
    "                # Wait for the page to load\n",
    "                WebDriverWait(driver, 10).until(EC.visibility_of_all_elements_located((By.XPATH, '//div[@data-testid=\"cellInnerDiv\"]//div[@class=\"css-146c3p1 r-dnmrzs r-1udh08x r-3s2u2q r-bcqeeo r-1ttztb7 r-qvutc0 r-37j5jr r-a023e6 r-rjixqe r-16dba41 r-18u37iz r-1wvb978\"]')))\n",
    "                WebDriverWait(driver, 10).until(lambda driver: len(driver.find_elements(By.XPATH, '//div[@data-testid=\"cellInnerDiv\"]//div[@class=\"css-146c3p1 r-dnmrzs r-1udh08x r-3s2u2q r-bcqeeo r-1ttztb7 r-qvutc0 r-37j5jr r-a023e6 r-rjixqe r-16dba41 r-18u37iz r-1wvb978\"]')) > last_height)\n",
    "\n",
    "                # Extract the usernames of following accounts\n",
    "                following_list = driver.find_elements(By.XPATH, '//div[@data-testid=\"cellInnerDiv\"]//div[@class=\"css-146c3p1 r-dnmrzs r-1udh08x r-3s2u2q r-bcqeeo r-1ttztb7 r-qvutc0 r-37j5jr r-a023e6 r-rjixqe r-16dba41 r-18u37iz r-1wvb978\"]//span[@class=\"css-1jxf684 r-bcqeeo r-1ttztb7 r-qvutc0 r-poiln3\"]')\n",
    "                accounts = [account.text for account in following_list]\n",
    "                for i in accounts:\n",
    "                    if i not in following_accounts:\n",
    "                        following_accounts.append(i)\n",
    "\n",
    "            except StaleElementReferenceException:\n",
    "                #print(\"StaleElementReferenceException encountered. Retrying...\")\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            \n",
    "            except TimeoutException:\n",
    "                break\n",
    "        \n",
    "        following_accounts_dict[user] = following_accounts\n",
    "    driver.quit()\n",
    "    return following_accounts_dict\n",
    "\n",
    "# Call the function to scrape followed accounts\n",
    "following_accounts_dict = scrape_following(twitter_usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213db29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "following_accounts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54660c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing @ from each username\n",
    "following_accounts_dict_ = {key: [item.replace('@', '') for item in value] for key, value in following_accounts_dict.items()}\n",
    "following_accounts_dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8c69a",
   "metadata": {},
   "source": [
    "### Scraping Number of Followers using Snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the number of followers for each user\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "def get_follower_count(screen_name):\n",
    "    try:\n",
    "        user = sntwitter.TwitterUserScraper(screen_name).entity\n",
    "        return user.followersCount\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# List of Twitter accounts\n",
    "no_of_followers_list_=[]\n",
    "for i in following_accounts_dict_:\n",
    "    twitter_accounts = following_accounts_dict_[i]\n",
    "    no_of_followers_ ={}\n",
    "    for account in twitter_accounts:\n",
    "        count = get_follower_count(account)\n",
    "        if count is not None:\n",
    "            no_of_followers_[account]=count\n",
    "    no_of_followers_list_.append(no_of_followers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_followers_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4cc032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 accounts in following list of each user\n",
    "accounts_to_scrape_ = []\n",
    "for i in range(len(no_of_followers_list_)):\n",
    "    following_df = pd.DataFrame({'username': list(no_of_followers_list_[i].keys()), 'followers_count': list(no_of_followers_list_[i].values())}).sort_values(by='followers_count', ascending=False).reset_index(drop=True)\n",
    "    top10_accounts = following_df['username'][0:10].tolist()\n",
    "    accounts_to_scrape_ += top10_accounts\n",
    "    \n",
    "print(accounts_to_scrape_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eea183",
   "metadata": {},
   "source": [
    "### Scraping Tweets using Ntscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e11c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntscraper import Nitter\n",
    "from IPython.display import FileLink\n",
    "scraper = Nitter(log_level=1, skip_instance_check = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scarpe the tweets \n",
    "def get_tweets(name, modes, no):\n",
    "    tweets = scraper.get_tweets(name, mode = modes, number = no)\n",
    "    final_tweet = []\n",
    "    for tweet in tweets['tweets']:\n",
    "        data = [tweet['link'], tweet['text'], tweet['user']['name'], tweet['user']['profile_id'], tweet['date'], tweet['stats']['comments'],  tweet['stats']['retweets'],  tweet['stats']['quotes'], tweet['stats']['likes']]\n",
    "        final_tweet.append(data)\n",
    "    return final_tweet\n",
    "\n",
    "# Function to extract Twitter account name from URL\n",
    "import re\n",
    "def twitter_account_name(url):\n",
    "    match = re.search(r'twitter\\.com/([^/]+)/', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# Creating a dataframe to store the tweets\n",
    "complete_df = pd.DataFrame(columns = ['link', 'text', 'user_name', 'profile_id', 'date', 'no_comments', 'no_retweets', 'no_quotes', 'no_likes'])\n",
    "\n",
    "# Calling the above function to scrape the tweets of each user\n",
    "all_twitter_accounts = accounts_to_scrape_\n",
    "\n",
    "all_accounts=[]\n",
    "for accounts in all_twitter_accounts:\n",
    "    try:\n",
    "        data = get_tweets(accounts, 'user', 1000)\n",
    "        all_accounts.append(data)\n",
    "        time.sleep(5)\n",
    "    except IndexError as e:\n",
    "        print(\"IndexError occured. Retrying......\")\n",
    "        time.sleep(5)\n",
    "        data = get_tweets(accounts, 'user', 1000)\n",
    "        all_accounts.append(data)\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "for account in all_accounts:\n",
    "    # Convert the account dictionary to a DataFrame and concatenate it with the main DataFrame\n",
    "    df = pd.DataFrame(account, columns = ['link', 'text', 'user_name', 'profile_id', 'date', 'no_comments', 'no_retweets', 'no_quotes', 'no_likes'])\n",
    "    complete_df = pd.concat([complete_df, df], ignore_index=True)\n",
    "\n",
    "# Apply the twitter_account_name function to the 'link' column to extract account name\n",
    "complete_df['account_name'] = complete_df['link'].apply(twitter_account_name)\n",
    "\n",
    "# Scraping the followers count of each unique account\n",
    "unique_accounts = complete_df['account_name'].unique()\n",
    "accounts_followers_dict_={}\n",
    "for name in unique_accounts:\n",
    "    count = get_follower_count(name)\n",
    "    accounts_followers_dict_[name]=count\n",
    "    \n",
    "# Appending the followers count to dataframe\n",
    "complete_df['followers'] = complete_df['account_name'].map(accounts_followers_dict_)\n",
    "\n",
    "# Dropping rows with na values\n",
    "complete_df = complete_df.dropna()\n",
    "\n",
    "# Converting Dataframe to csv\n",
    "complete_df.to_csv('twitter_data.csv', index=False)\n",
    "FileLink('twitter_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb074e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
